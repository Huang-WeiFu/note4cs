- [urllib库的基本使用](#urllib库的基本使用)
  - [request](#request)
    - [urlopen](#urlopen)
      - [获取response中的参数](#获取response中的参数)
      - [data参数传输表单数据](#data参数传输表单数据)
      - [超时时间timeout参数](#超时时间timeout参数)
      - [使用try except来对超时时间异常进行处理](#使用try-except来对超时时间异常进行处理)
    - [使用Request类来构建请求](#使用request类来构建请求)
      - [构建请求](#构建请求)
      - [构建多个参数的request](#构建多个参数的request)
    - [高级用法](#高级用法)
      - [使用代理方式构建请求](#使用代理方式构建请求)
      - [需要认证页面未认证会发生的错误](#需要认证页面未认证会发生的错误)
      - [添加认证所需条件](#添加认证所需条件)
      - [获取cookie](#获取cookie)
      - [将cookie存储为文件](#将cookie存储为文件)
      - [cookie存储为不同形式的文件](#cookie存储为不同形式的文件)
      - [使用已有cookie访问网页](#使用已有cookie访问网页)
  - [异常的处理](#异常的处理)
    - [使用URLError处理异常](#使用urlerror处理异常)
    - [使用HttpError处理异常](#使用httperror处理异常)
    - [先捕获子类再捕获父类的错误](#先捕获子类再捕获父类的错误)
    - [根据错误的类型返回值](#根据错误的类型返回值)
  - [解析](#解析)
    - [urlparse](#urlparse)
    - [urlunparse](#urlunparse)
    - [urlsplit](#urlsplit)
    - [urlunsplit](#urlunsplit)
    - [urljoin](#urljoin)
    - [urlencode](#urlencode)
    - [parse_qs](#parse_qs)
    - [pare_qsl](#pare_qsl)
    - [quote](#quote)
    - [unquote](#unquote)
  - [爬虫机器人](#爬虫机器人)
    - [读取爬虫机器人文件](#读取爬虫机器人文件)
    - [使用parse方法读取爬虫机器人文件](#使用parse方法读取爬虫机器人文件)

# urllib库的基本使用

>     利用urllib库可以实现HTTP请求的发送，不需要关心HTTP协议本身甚至更低层的实现，只需要指定请求的URL、请求头、请求体等信息。urllib还可以把服务器返回的响应转化为Python对象，可以通过该对象获取响应的相关信息。

- urllib：Python内置Http请求库，不需要额外安装
  
  - request：模拟请求的发送
  
  - error：异常处理模块
  
  - parse：工具模块，提供许多处理URL的方法
  
  - robotparser：识别网站的robots.txt文件，判断哪些网站是否可以爬

## request

### urlopen

```python
import urllib.request

# 利用urllib库爬取python官网数据
response = urllib.request.urlopen("https://www.python.org")
# 以utf-8的解码方式解码html
print(response.read().decode('utf-8'))
# 查看获取的数据的类型
print(type(response))
```

#### 获取response中的参数

>     利用urllib将服务器返回的响应转化为Python对象，通过该对象获取响应的内容。

```python
import urllib.request

response = urllib.request.urlopen("https://www.python.org")
# 状态码
print(response.status)
# 所有请求头
print(response.getheaders())  
# 获取请求头中键为Server的数据
print(response.getheader('Server'))
```

#### data参数传输表单数据

>     data参数为表单数据，是可选的，添加该参数时需将其转化为字节流编码格式，即bytes类型，并且使用该参数，则请求方式变为POST。

```python
import urllib.request
import urllib.parse

data = bytes(urllib.parse.urlencode({"name": "germey"}), encoding='utf-8')
response = urllib.request.urlopen("https://www.httpbin.org/post", data=data)
print(response.read().decode('utf-8'))


'''
输出结果
{
  "args": {}, 
  "data": "", 
  "files": {}, 
  "form": {     出现了form字段，采用了post方式提交了表单
    "name": "germey"
  }, 
  "headers": {
    "Accept-Encoding": "identity", 
    "Content-Length": "11", 
    "Content-Type": "application/x-www-form-urlencoded", 
    "Host": "www.httpbin.org", 
    "User-Agent": "Python-urllib/3.9", 
    "X-Amzn-Trace-Id": "Root=1-62b4330f-2883099517724ae45ec14a77"
  }, 
  "json": null, 
  "origin": "36.251.139.168", 
  "url": "https://www.httpbin.org/post"
}
'''
```

#### 超时时间timeout参数

>     设置超时时间后，在该时间内未得到服务器响应就会抛出异常

```python
import urllib.request

# 使用timeout参数设置超时时间，单位为秒
response = urllib.request.urlopen('https://www.httpbin.org/get', timeout=0.1)
print(response.read())
```

#### 使用try except来对超时时间异常进行处理

```python
import urllib.request
import socket

try:
    response = urllib.request.urlopen('https://www.httpbin.org/get', timeout=0.1)
except urllib.error.URLError as e:
    if isinstance(e.reason, socket.timeout):
        print("TIME OUT")
```

### 使用Request类来构建请求

```python
# Request的构造方法
class Request:

    def __init__(self, 
                 url, # 请求路径
                 data=None, # 表单数据，必须是bytes类型，可用urllib.parse.urlencode方法编码
                 headers={}, # 请求头，最常用是使用User-Agent伪装成浏览器
                 origin_req_host=None, # 请求方的host或ip
                 unverifiable=False, # 权限相关
                 method=None # 方法GET、POST、PUT等
                 ):
```

#### 构建请求

```python
import urllib.request

request = urllib.request.Request('https://python.org')
response = urllib.request.urlopen(request)
print(response.read().decode('utf-8'))
```

#### 构建多个参数的request

```python
import urllib.request
import urllib.parse

# 请求路径
url = 'https://httpbin.org/post'
# 请求头
headers = {
    'User-Agent': 'Mozilla/4.0(compatible; MSIE 5.5; Windows NT',
    'Host': 'www.httpbin.org'
}
# 表单数据
dict = {"name": "germey"}
data = bytes(urllib.parse.urlencode(dict), encoding='utf-8')
# 构建请求
req = urllib.request.Request(url=url, data=data, headers=headers, method='POST')
# 发送请求
response = urllib.request.urlopen(req)
print(response.read().decode('utf-8'))
'''
请求结果
{
  "args": {}, 
  "data": "", 
  "files": {}, 
  "form": {
    "name": "germey"
  }, 
  "headers": {
    "Accept-Encoding": "identity", 
    "Content-Length": "11", 
    "Content-Type": "application/x-www-form-urlencoded", 
    "Host": "www.httpbin.org", 
    "User-Agent": "Mozilla/4.0(compatible; MSIE 5.5; Windows NT", 
    "X-Amzn-Trace-Id": "Root=1-62b445b6-6d1224140ec990f4145feb3c"
  }, 
  "json": null, 
  "origin": "36.251.139.168", 
  "url": "https://www.httpbin.org/post"
}
'''
```

### 高级用法

#### 使用代理方式构建请求

```python
from urllib.error import URLError
from urllib.request import ProxyHandler, build_opener

# 构建代理参数
proxy_handler = ProxyHandler({
    "http": "http://127.0.0.1:8080",
    "https": "https://127.0.0.1:8080"
})
# 利用handler类来创建opener类
# opener类提供的open方法返回的响应类型与urlopen相同
opener = build_opener(proxy_handler)
try:
    response = opener.open("https://www.baidu.com")
    print(response.read().decode('utf-8'))
except URLError as e:
    print(e.reason)
```

#### 需要认证页面未认证会发生的错误

```python
import urllib.request

# 若在需要认证的页面未认证则会报401错误
url = "https://ssr3.scrape.center/"
resp = urllib.request.urlopen(url)
print(resp)
'''
urllib.error.HTTPError: HTTP Error 401: Authorization Required
'''
```

#### 添加认证所需条件

```python
from urllib.request import HTTPPasswordMgrWithDefaultRealm, HTTPBasicAuthHandler, build_opener
from urllib.error import URLError

# 需要认证的页面参数的构建
username = "admin"
password = "admin"
url = "https://ssr3.scrape.center/"

p = HTTPPasswordMgrWithDefaultRealm()
p.add_password(None, url, username, password)
auth_handler = HTTPBasicAuthHandler(p)
opener = build_opener(auth_handler)

try:
    result = opener.open(url)
    html = result.read().decode("utf-8")
    print(html)
except URLError as e:
    print(e.reason)
```

#### 获取cookie

```python
import http.cookiejar, urllib.request

cookie = http.cookiejar.CookieJar()
handler = urllib.request.HTTPCookieProcessor(cookie)
opener = urllib.request.build_opener(handler)
response = opener.open("https://www.baidu.com")
for item in cookie:
    print(item.name, '=', item.value)
```

#### 将cookie存储为文件

```python
import http.cookiejar, urllib.request

file_name = "cookie_Mozilla.txt"
cookie = http.cookiejar.MozillaCookieJar(file_name)
handler = urllib.request.HTTPCookieProcessor(cookie)
opener = urllib.request.build_opener(handler)
response = opener.open("https://www.baidu.com")
cookie.save(ignore_discard=True, ignore_expires=True)


"""
文件内容
# Netscape HTTP Cookie File
# http://curl.haxx.se/rfc/cookie_spec.html
# This is a generated file!  Do not edit.

.baidu.com    TRUE    /    FALSE    1687518891    BAIDUID    E83C7CED4E51D688152A56EEED1DFFAD:FG=1
.baidu.com    TRUE    /    FALSE    3803466538    BIDUPSID    E83C7CED4E51D6886D57A74AE9BCBF57
.baidu.com    TRUE    /    FALSE    3803466538    PSTM    1655982891
www.baidu.com    FALSE    /    FALSE    1655983191    BD_NOT_HTTPS    1

"""
```

#### cookie存储为不同形式的文件

```python
import http.cookiejar, urllib.request

file_name = "cookie_LWP.txt"
cookie = http.cookiejar.LWPCookieJar(file_name)
handler = urllib.request.HTTPCookieProcessor(cookie)
opener = urllib.request.build_opener(handler)
response = opener.open("https://www.baidu.com")
cookie.save(ignore_discard=True, ignore_expires=True)

"""
#LWP-Cookies-2.0
Set-Cookie3: BAIDUID="FC263AFD987D38FA3BE54993DC16F9C0:FG=1"; path="/"; domain=".baidu.com"; path_spec; domain_dot; expires="2023-06-23 11:18:58Z"; comment=bd; version=0
Set-Cookie3: BIDUPSID=FC263AFD987D38FA9D7A7F29A9DD936C; path="/"; domain=".baidu.com"; path_spec; domain_dot; expires="2090-07-11 14:33:05Z"; version=0
Set-Cookie3: PSTM=1655983138; path="/"; domain=".baidu.com"; path_spec; domain_dot; expires="2090-07-11 14:33:05Z"; version=0
Set-Cookie3: BD_NOT_HTTPS=1; path="/"; domain="www.baidu.com"; path_spec; expires="2022-06-23 11:23:58Z"; version=0
"""
```

#### 使用已有cookie访问网页

```python
import http.cookiejar, urllib.request

cookie = http.cookiejar.LWPCookieJar()
cookie.load('cookie_LWP.txt', ignore_discard=True, ignore_expires=True)
handler = urllib.request.HTTPCookieProcessor(cookie)
opener = urllib.request.build_opener(handler)
response = opener.open("https://www.baidu.com")
print(response.read().decode("utf-8"))
```

## 异常的处理

>     若发生异常，不处理则可能会因为报错而终止

### 使用URLError处理异常

```python
from urllib import request
from urllib import error as err

try:
    response = request.urlopen("https://cuiqingcai.com/404")
except err.URLError as e:
    print(e.reason)
```

### 使用HttpError处理异常

```python
from urllib import request
from urllib import error as err

try:
    response = request.urlopen("https://cuiqingcai.com/404")
except err.HTTPError as e:
    print(e.reason, e.code, e.headers, sep="\n")
```

### 先捕获子类再捕获父类的错误

```python
from urllib import request
from urllib import error as err

try:
    response = request.urlopen("https://cuiqingcai.com/404")
except err.HTTPError as e:
    print(e.reason, e.code, e.headers, sep="\n")
except err.URLError as e:
    print(e.reason)
else:
    print("Request Successful")
```

### 根据错误的类型返回值

```python
from urllib import request
from urllib import error as err
import socket

try:
    response = request.urlopen("https://baidu.com", timeout=0.01)
except err.URLError as e:
    print(type(e.reason))
    if isinstance(e.reason, socket.timeout):
        print('TIME OUT')
```

## 解析

>     urllib库提供的parse模块用以实现URL各部分的抽取、合并以及链接转换

### urlparse

```python
from urllib.parse import urlparse

"""
def urlparse(url, 请求路径
             scheme='', 协议
             allow_fragments=True 忽略锚点
             ):
"""
result = urlparse("https://www.baidu.com/index.html;usr?id=5#commit")
"""
ParseResult(scheme='https', 协议
            netloc='www.baidu.com', 域名
            path='/index.html', 路径
            params='usr', 参数
            query='id=5', 查询条件
            fragment='commit' 用于定位页面内部的下拉位置
            )
"""
print(type(result))
print(result)

# 返回的对象是个元组
print(result.scheme, result[0], result.netloc, result[1], sep='\n')
'''
https
https
www.baidu.com
www.baidu.com
'''
```

### urlunparse

```python
from urllib.parse import urlunparse

# 将参数反解析成为url
data = ['https', 'www.baidu.com', 'index.html', 'user', 'a=6', 'comment']
print(urlunparse(data))
'''
https://www.baidu.com/index.html;user?a=6#comment
'''
```

### urlsplit

```python
from urllib.parse import urlsplit

result = urlsplit("https://www.baidu.com/index.html;usr?id=5#comment")
print(result)
'''
SplitResult(scheme='https', netloc='www.baidu.com', path='/index.html;usr', query='id=5', fragment='comment')
'''
# 返回结果同样是元组
print(result.scheme, result[0])
'''
https https
'''
```

### urlunsplit

```python
from urllib.parse import urlunsplit

# 与urlunparse方法类似，但参数长度必须为5
data = ['https', 'www.baidu.com', 'index.html', 'a=6', 'comment']
print(urlunsplit(data))
'''
https://www.baidu.com/index.html?a=6#comment
'''
```

### urljoin

```python
from urllib.parse import urljoin

# 将两个url中的协议、域名、路径等拼接补全，以后面的第二个参数为准
print(urljoin('https://www.baidu.com', 'FAQ.html'))
print(urljoin('https://www.baidu.com', 'https://cuiqingcai.com/FAQ.html'))
print(urljoin('https://www.baidu.com/about.html', 'https://cuiqingcai.com/FAQ.html'))
print(urljoin('https://www.baidu.com/about.html', 'https://cuiqingcai.com/FAQ.html?question=2'))
print(urljoin('https://www.baidu.com?wd=abc', 'https://cuiqingcai.com/index.php'))
print(urljoin('https://www.baidu.com', '?category=2#comment'))
print(urljoin('www.baidu.com', '?category=2#comment'))
print(urljoin('www.baidu.com#comment', '?category=2'))
'''
https://www.baidu.com/FAQ.html
https://cuiqingcai.com/FAQ.html
https://cuiqingcai.com/FAQ.html
https://cuiqingcai.com/FAQ.html?question=2
https://cuiqingcai.com/index.php
https://www.baidu.com?category=2#comment
www.baidu.com?category=2#comment
www.baidu.com?category=2
'''
```

### urlencode

```python
from urllib.parse import urlencode

params = {
    'name': 'germey',
    'age': 25
}
base_url = 'https://www.baidu.com?'
# 参数转化为路径参数
url = base_url + urlencode(params)
print(url)
'''
https://www.baidu.com?name=germey&age=25
'''
```

### parse_qs

```python
from urllib.parse import parse_qs

query = 'name=germey&age=25'
# 路径中的参数转化为字典
print(parse_qs(query))
'''
{'name': ['germey'], 'age': ['25']}
'''
```

### pare_qsl

```python
from urllib.parse import parse_qsl

query = 'name=germey&age=25'
# 路径中的参数转化为元组
print(parse_qsl(query))
'''
[('name', 'germey'), ('age', '25')]
'''
```

### quote

```python
from urllib.parse import quote


keyword = '壁纸'
# 将中文进行url编码
url = 'https://www.baidu.com/s?wd=' + quote(keyword)
print(url)
'''
https://www.baidu.com/s?wd=%E5%A3%81%E7%BA%B8
'''
```

### unquote

```python
from urllib.parse import unquote

url = "https://www.baidu.com/s?wd=%E5%A3%81%E7%BA%B8"
# 将中文进行url解码
print(unquote(url))
'''
https://www.baidu.com/s?wd=壁纸
'''
```

## 爬虫机器人

>     Robots协议用来告诉爬虫和搜索引擎哪些页面抓取、哪些不可以。

### 读取爬虫机器人文件

```python
from urllib.robotparser import RobotFileParser

rp = RobotFileParser()
rp.set_url('https://www.baidu.com/robots.txt')
rp.read()
print(rp.can_fetch('Baiduspider', 'https://www.baidu.com'))
print(rp.can_fetch('Baiduspider', 'https://www.baidu.com/homepage/'))
print(rp.can_fetch('Googlebot', 'https://www.baidu.com/homepage/'))
```

### 使用parse方法读取爬虫机器人文件

```python
import urllib.request
from urllib.robotparser import RobotFileParser

rp.parse(urllib.request.urlopen("https://www.baidu.com/robots.txt").read().decode('utf-8').split("\n"))
print(rp.can_fetch('Baiduspider', 'https://www.baidu.com'))
print(rp.can_fetch('Baiduspider', 'https://www.baidu.com/homepage/'))
print(rp.can_fetch('Googlebot', 'https://www.baidu.com/homepage/'))
```
