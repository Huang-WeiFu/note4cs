- [request库的基本使用](#request库的基本使用)
  - [实例引入](#实例引入)
  - [GET请求](#get请求)
    - [基本实例](#基本实例)
      - [发送GET请求](#发送get请求)
      - [GET请求添加参数](#get请求添加参数)
      - [GET方法返回类型](#get方法返回类型)
    - [抓取网页](#抓取网页)
    - [抓取二进制数据](#抓取二进制数据)
    - [添加请求头](#添加请求头)
  - [POST请求](#post请求)
  - [响应](#响应)
  - [高级用法](#高级用法)
    - [文件上传](#文件上传)
    - [Cookie设置](#cookie设置)
    - [Session维持](#session维持)
      - [cookie模拟同一会话](#cookie模拟同一会话)
      - [session模拟同一会话](#session模拟同一会话)
    - [SSL证书验证](#ssl证书验证)
      - [使用urllib3忽略警告](#使用urllib3忽略警告)
      - [捕获警告到日志](#捕获警告到日志)
      - [使用本地证书作为客户端证书](#使用本地证书作为客户端证书)
    - [超时设置](#超时设置)
    - [身份认证](#身份认证)
      - [简单认证](#简单认证)
      - [OAuth认证](#oauth认证)
    - [代理设置](#代理设置)
      - [socks协议代理](#socks协议代理)
    - [Prepared Request](#prepared-request)

# request库的基本使用

>     request库不是Python内置库，需要使用`pip3 install requests`进行安装

## 实例引入

>     request库进行get请求

```python
import requests

r = requests.get('https://www.baidu.com')
print(type(r))
print(r.status_code)
print(type(r.text))
print(r.text[:100])
print(r.cookies)
'''
<class 'requests.models.Response'>
200
<class 'str'>
<!DOCTYPE html>
<!--STATUS OK--><html> <head><meta http-equiv=content-type content=text/html;charse
<RequestsCookieJar[<Cookie BDORZ=27315 for .baidu.com/>]>
'''
```

>     request库用于其他请求类型依旧可以使用一句话完成

```python
import requests

r0 = requests.get("https://httpbin.org/get")
r1 = requests.post("https://httpbin.org/post")
r2 = requests.put("https://httpbin.org/put")
r3 = requests.delete("https://httpbin.org/delete")
r4 = requests.patch("https://httpbin.org/patch")
```

## GET请求

### 基本实例

#### 发送GET请求

```python
import requests

r = requests.get("https://www.httpbin.org/get")
print(r.text)
'''
{
  "args": {}, 
  "headers": {
    "Accept": "*/*", 
    "Accept-Encoding": "gzip, deflate", 
    "Host": "httpbin.org", 
    "User-Agent": "python-requests/2.28.0", 
    "X-Amzn-Trace-Id": "Root=1-62b56f8c-0237f5ef640bfce32f080b85"
  }, 
  "origin": "36.251.139.168", 
  "url": "https://www.httpbin.org/get"
}
'''
```

#### GET请求添加参数

>     GET请求参数若于URL后手动拼接着实不优雅，利用requests库提供的API自动拼接可以实现添加参数功能。

```python
import request

data = {
    "name": "germey",
    "age": 25
}
r = requests.get("https://www.httpbin.org/get", params=data)
print(r.text)
'''
{
  "args": {
    "age": "25", 
    "name": "germey"
  }, 
  "headers": {
    "Accept": "*/*", 
    "Accept-Encoding": "gzip, deflate", 
    "Host": "www.httpbin.org", 
    "User-Agent": "python-requests/2.28.0", 
    "X-Amzn-Trace-Id": "Root=1-62b57024-66bcced34590f8fb42a26bfe"
  }, 
  "origin": "36.251.139.168", 
  "url": "https://www.httpbin.org/get?name=germey&age=25"
}
'''
```

#### GET方法返回类型

>     网页的返回类型是str类型，但它是JSON格式的

```python
import requests

data = {
    "name": "germey",
    "age": 25
}
r = requests.get("https://www.httpbin.org/get", params=data)
print(r.json())
print(type(r.json()))
'''
<class 'str'>
 {'args': {'age': '25', 'name': 'germey'}, 'headers': {'Accept': '*/*', 'Accept-Encoding': 'gzip, 
deflate', 'Host': 'www.httpbin.org', 'User-Agent': 'python-requests/2.28.0', 'X-Amzn-Trace-Id': 
'Root=1-62b5705e-4761e7be62f079184aae0480'}, 'origin': '36.251.139.168', 
'url': 'https://www.httpbin.org/get?name=germey&age=25'} 
<class 'dict'> 
'''
```

### 抓取网页

>     抓取网页数据后，若数据为HTML文档，可以通过正则表达式进行数据筛选

```python
import requests
import re

# 正则表达式抓取
r = requests.get("https://ssr1.scrape.center/")
pattern = re.compile('<h2.*?>(.*?)</h2>', re.S)
title = re.findall(pattern, r.text)
print(title)
'''
['霸王别姬 - Farewell My Concubine', '这个杀手不太冷 - Léon', '肖申克的救赎 - The Shawshank Redemption', '泰坦尼克号 - Titanic', 
'罗马假日 - Roman Holiday', '唐伯虎点秋香 - Flirting Scholar', '乱世佳人 - Gone with the Wind', '喜剧之王 - The King of Comedy', 
'楚门的世界 - The Truman Show', '狮子王 - The Lion King'] 
'''
```

### 抓取二进制数据

>     若抓取数据为图片、音频、视频等文件，均为二进制码组成，抓取它们必须拿到它们的二进制数据

```python
import requests

# 抓取二进制数据
r = requests.get("https://scrape.center/favicon.ico")
print(r.text)
print(r.content)
# 抓取二进制数据后保存
with open('favicon.ico', 'wb') as f:
    f.write(r.content)
```

### 添加请求头

```python
import requests

# 设置request请求头
headers = {
    'User-Agent': 'Mozilla/5.0(Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) '
                  'Chrome/52.0.2743.116 Safari/ 537.36 '
}
r = requests.get("https://ssr1.scrape.center/", headers=headers)
print(r.text)
```

## POST请求

```python
import requests

data = {'name': 'getmey', "age": "25"}
r = requests.post('https://www.httpbin.org/post', data=data)
print(r.text)
'''
{
  "args": {}, 
  "data": "", 
  "files": {}, 
  "form": {
    "age": "25", 
    "name": "getmey"
  }, 
  "headers": {
    "Accept": "*/*", 
    "Accept-Encoding": "gzip, deflate", 
    "Content-Length": "18", 
    "Content-Type": "application/x-www-form-urlencoded", 
    "Host": "www.httpbin.org", 
    "User-Agent": "python-requests/2.28.0", 
    "X-Amzn-Trace-Id": "Root=1-62cc34b4-6afca0f24c34a1270071ee23"
  }, 
  "json": null, 
  "origin": "58.23.73.35", 
  "url": "https://www.httpbin.org/post"
}
'''
```

## 响应

```python
import requests

r = requests.get("https://ssr1.scrape.center/")
print(type(r.status_code), r.status_code)
print(type(r.headers), r.headers)
print(type(r.cookies), r.cookies)
print(type(r.url), r.url)
print(type(r.history), r.history)
'''
<class 'int'> 200 <class 'requests.structures.CaseInsensitiveDict'> {'Date': 'Mon, 11 Jul 2022 14:35:52 GMT', 
'Content-Type': 'text/html; charset=utf-8', 'Content-Length': '41667', 'Connection': 'keep-alive', 'Server': 'Lego 
Server', 'Cache-Control': 'max-age=600', 'Expires': 'Mon, 11 Jul 2022 14:45:51 GMT', 'X-Frame-Options': 'DENY', 
'X-Content-Type-Options': 'nosniff', 'Strict-Transport-Security': 'max-age=15724800; includeSubDomains', 
'X-NWS-LOG-UUID': '91858fb4-3dc8-45ec-ac52-09281eefcda1', 'X-Cache-Lookup': 'Cache Miss', 'X-Daa-Tunnel': 
'hop_count=1'} <class 'requests.cookies.RequestsCookieJar'> <RequestsCookieJar[]> <class 'str'> 
https://ssr1.scrape.center/ <class 'list'> [] 
'''
```

## 高级用法

### 文件上传

```python
import requests

files = {'file': open('favicon.ico', 'rb')}
r = requests.post('https://www.httpbin.org/post', files=files)
print(r.text)
'''
{
  "args": {}, 
  "data": "", 
  "files": {
    "file": "data:application/octet-stream;base64,AAABAAEAICAAAAEAIACoE..." # 使用单独字段file来标识文件
  }, 
  "form": {}, 
  "headers": {
    "Accept": "*/*", 
    "Accept-Encoding": "gzip, deflate", 
    "Content-Length": "4433", 
    "Content-Type": "multipart/form-data; boundary=8a139e75dbe61db35a18c6f5bf6170ff", 
    "Host": "www.httpbin.org", 
    "User-Agent": "python-requests/2.28.0", 
    "X-Amzn-Trace-Id": "Root=1-62cc3651-5bb478325d15f34a707f235d"
  }, 
  "json": null, 
  "origin": "58.23.73.35", 
  "url": "https://www.httpbin.org/post"
}
'''
```

### Cookie设置

>     获取cookie

```python
import requests

r = requests.get('https://www.baidu.com')
print(r.cookies)
for key, value in r.cookies.items():
    print(key + '=' + value)
'''
<RequestsCookieJar[<Cookie BDORZ=27315 for .baidu.com/>]>
BDORZ=27315
'''
```

>     cookie模拟登陆

```python

```

### Session维持

#### cookie模拟同一会话

```python
import requests

r = requests.get('https://www.httpbin.org/cookies/set/number/123456789')
print(r.text)
r1 = requests.get("https://www.httpbin.org/cookies")
print(r1.text)
```

#### session模拟同一会话

```python
import requests

s = requests.Session()
s.get("https://www.httpbin.org/cookies/set/number/123456789")
r = s.get("https://www.httpbin.org/cookies")
print(r.text)
```

### SSL证书验证

>     在使用HTTPS协议下，网站未设置HTTPS证书或证书不被CA机构认可，就会出现SSL证书错误的提示。

```python
import requests

response = requests.get('https://ssr2.scrape.center/')
print(response.status_code)
# verify参数控制是否验证证书
response = requests.get('https://ssr2.scrape.center/', verify=False)
print(response.status_code)
```

#### 使用urllib3忽略警告

```python
import requests
from requests.packages import urllib3

urllib3.disable_warnings()
response = requests.get('https://ssr2.scrape.center/', verify=False)
print(response.status_code)
```

#### 捕获警告到日志

```python
import requests
import logging

# 捕获警告到日志以忽略警告
logging.captureWarnings(True)
response = requests.get('https://ssr2.scrape.center/', verify=False)
print(response.status_code)
```

#### 使用本地证书作为客户端证书

```python
import requests

# 指定本地证书作为用户端证书，key必须为解密状态
response = requests.get('https://ssr2.scrape.center/', cert=('/xxx/xx.crt', 'xxx.key'))
```

### 超时设置

```python
import requests

# 超时设置
r = requests.get('https://httpbin.org/get', timeout=1)
print(r.status_code)
# 分别设置连接和读取的timeout
r = requests.get('https://httpbin.org/get', timeout=(5, 30))
# 不设置或者设置为None标识永不返回错误
r = requests.get('https://httpbin.org/get', timeout=None)
r = requests.get('https://httpbin.org/get')
```

### 身份认证

#### 简单认证

```python
import requests
from requests.auth import HTTPBasicAuth

r = requests.get('https://ssr3.scrape.center/', auth=HTTPBasicAuth('admin', "admin"))
print(r.status_code)
# 简略写法
r = requests.get('https://ssr3.scrape.center/', auth=('admin', 'admin'))
print(r.status_code)
```

#### OAuth认证

>     OAuth认证需要安装oauth包，oauth包不是Python内置库，需要执行`pip3 install requests_oauthlib`

```python
import requests
from requests_oauthlib import OAuth1

url = 'https://api.twitter.com/1.1/account/verify_credentials.json'
auth = OAuth1("KEY", 'SECRET', 'OAUTH_TOKEN', 'OAUTH_TOKEN_SECRET')
requests.get(url, auth=auth)
```

### 代理设置

>     大规模爬取数据时，可能会弹出验证码、跳转登陆、甚至封禁IP无法访问，可以通过代理解决此问题。

```python
import requests

# 代理设置
# 此代理可能无效，网上寻找有效代理
proxies = {
    'http': 'http://10.10.10.10:1080',
    'https': 'https://10.10.10.10:1080'
}
requests.get('https://httpbin.org/get', proxies=proxies)
# 需要身份验证的代理
proxies = {
    "https": "http://user:password@10.10.10.10:1080/",
}
requests.get("https://httpbin.org/get", proxies=proxies)
```

#### socks协议代理

>     requests库支持SOCKS协议的代理，需要安装socks库，执行`pip3 install requests[socks]`

```python
import requests

# socks协议代理
proxies={
    'http':'socks5://user:password@host:port',
    'http2':'socks5://user:password@host:port'
}
requests.get("https://httpbin.org/get", proxies=proxies)
```

### Prepared Request

>     requests发送请求时，使用的Request对象时Prepared Request

```python
# 使用Prepared Request构建请求
from requests import Request, Session

url = 'https://www.httpbin.org/post'
data = {'name': 'germey'}
headers = {
    'User-Agent': 'Mozilla/5.0(Mocintosh;Intel Mac OS X 10_11_4 AppleWebKit/537.36(KHTML, like Gecko) '
                  'Chrome/53.0.2785.116 Safari/537.36 '
}
s = Session()
req = Request('POST', url, data=data, headers=headers)
prepped = s.prepare_request(req)
r = s.send(prepped)
print(r.text)
'''
{
  "args": {}, 
  "data": "", 
  "files": {}, 
  "form": {
    "name": "germey"
  }, 
  "headers": {
    "Accept": "*/*", 
    "Accept-Encoding": "gzip, deflate", 
    "Content-Length": "11", 
    "Content-Type": "application/x-www-form-urlencoded", 
    "Host": "www.httpbin.org", 
    "User-Agent": "Mozilla/5.0(Mocintosh;Intel Mac OS X 10_11_4 AppleWebKit/537.36(KHTML, like Gecko) Chrome/53.0.2785.116 Safari/537.36", 
    "X-Amzn-Trace-Id": "Root=1-62cf8163-77b1032a232a21f26f90b727"
  }, 
  "json": null, 
  "origin": "58.23.73.35", 
  "url": "https://www.httpbin.org/post"
}
'''
```